{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a50ae3-2959-4140-8505-e52f89bc604f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Load the Hugging Face API token from a configuration file\n",
    "working_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "config_file_path = os.path.join(working_dir, \"config.json\")\n",
    "\n",
    "# Check if the configuration file exists\n",
    "if not os.path.exists(config_file_path):\n",
    "    raise FileNotFoundError(f\"Configuration file not found at {config_file_path}\")\n",
    "\n",
    "# Load the configuration file\n",
    "with open(config_file_path, \"r\") as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "# Get the API token from the configuration data\n",
    "huggingface_token = config_data.get(\"HUGGINGFACE_API_TOKEN\")\n",
    "if huggingface_token is None:\n",
    "    raise ValueError(\"HUGGINGFACE_API_TOKEN not found in the configuration file\")\n",
    "\n",
    "# Load the LLaMA 3.1 model\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=huggingface_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, token=huggingface_token, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configuring Streamlit page settings\n",
    "st.set_page_config(page_title=\"Llama Chat\", page_icon=\"ðŸ’¬\", layout=\"wide\")\n",
    "\n",
    "# Initialize chat session in Streamlit if not already present\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a super useful assistant that is able to provide relevant and factually accurate advice. Please do not provide any medical, legal, or financial advice. If you are unsure about a response, please let the user know.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Hello! I'm Llama3.1, a chatbot trained to provide information and answer questions. Feel free to ask me anything!\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "    .chat-container {\n",
    "        max-height: 60vh;\n",
    "        overflow-y: auto;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.subheader(\"Generation Parameters\")\n",
    "    temperature = st.slider(\n",
    "        \"Temperature\", min_value=0.1, max_value=1.0, value=0.7, step=0.1\n",
    "    )\n",
    "    top_k = st.slider(\"Top K\", min_value=1, max_value=100, value=50)\n",
    "    top_p = st.slider(\"Top P\", min_value=0.1, max_value=1.0, value=0.95, step=0.05)\n",
    "    max_new_tokens = st.slider(\n",
    "        \"Max New Tokens\", min_value=64, max_value=2048, value=256, step=64\n",
    "    )\n",
    "\n",
    "    st.button(\"Clear Chat\", on_click=lambda: st.session_state.pop(\"chat_history\", None))\n",
    "\n",
    "# Main chat interface\n",
    "st.title(\"ðŸ¤– Llama3.1 - ChatBot\")\n",
    "\n",
    "# Create a container for the scrollable chat history\n",
    "st.markdown('<div class=\"chat-container\">', unsafe_allow_html=True)\n",
    "chat_container = st.container()\n",
    "st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# Create a container for the fixed input field at the bottom\n",
    "input_container = st.container()\n",
    "\n",
    "# Display chat history in the scrollable container\n",
    "with chat_container:\n",
    "    for message in st.session_state.chat_history[1:]:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "# Input field for user's message (fixed at the bottom)\n",
    "with input_container:\n",
    "    user_prompt = st.chat_input(\"Ask Llama...\", key=\"user_input\")\n",
    "\n",
    "if user_prompt:\n",
    "    # Add user's message to chat and display it\n",
    "    with chat_container:\n",
    "        st.chat_message(\"user\").markdown(user_prompt)\n",
    "    st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # Prepare the input for the model\n",
    "    input_text = \"\"\n",
    "    for message in st.session_state.chat_history:\n",
    "        input_text += f\"{message['role']}: {message['content']}\\n\"\n",
    "    input_text += \"assistant:\"\n",
    "\n",
    "    # Generate a response using the LLaMA 3.1 model and measure the time it takes\n",
    "    start_time = time.time()\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    assistant_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the time taken for response generation\n",
    "    generation_time = end_time - start_time\n",
    "    print(f\"Response generated in {generation_time:.2f} seconds\")\n",
    "\n",
    "    # Extract the assistant's response correctly\n",
    "    assistant_response = assistant_response.split(\"assistant:\")[-1].strip()\n",
    "\n",
    "    st.session_state.chat_history.append(\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "    )\n",
    "\n",
    "    # Display Llama's response\n",
    "    with chat_container:\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be34b98-e65d-4293-af32-81d9682111bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/10.1 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.0/10.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.4/2.4 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.27.0 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2dc362-91d5-4b75-ad7e-24da36ed0b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
